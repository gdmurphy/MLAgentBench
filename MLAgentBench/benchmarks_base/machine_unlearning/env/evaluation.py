"""Evaluation code for machine unlearning methods."""

import os
import time
import inspect
import nbformat
import requests
from copy import deepcopy
from typing import Callable
from tqdm import tqdm
import json
import shutil

import numpy as np

from sklearn import linear_model, model_selection
from sklearn.metrics import make_scorer, accuracy_score

import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, Subset, random_split

import kaggle
from kaggle.api.kaggle_api_extended import KaggleApi

from load_cifar_script import get_cifar10_data, get_cifar10_pretrained_models

# Constants
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_MODELS = 10  # Number of models for evaluation 
COMPETITION_NAME = "neurips-2023-machine-unlearning"

# Check and initialize Kaggle API
if not os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):
    raise RuntimeError('Kaggle API token not found. Please generate token from Kaggle account settings and place at ~/.kaggle/kaggle.json')

api = KaggleApi()
api.authenticate()

def evaluate_model(method, phase: str = "dev"):
    """Evaluate an unlearning method.
    
    Args:
        method: The unlearning method to evaluate
        phase: Either "dev" or "test" phase
        
    Returns:
        Dict containing evaluation metrics
    """
    print(f"Starting evaluation for {method.get_name()} in {phase} phase")
    
    if phase == "dev":
        return _evaluate_dev(method)
    elif phase == "test":
        return _evaluate_test(method)
    else:
        raise ValueError(f"Invalid phase: {phase}")
    
###### DEV PHASE ######

def accuracy(net, loader):
    """Return accuracy on a dataset given by the data loader."""
    correct = 0
    total = 0
    batch_count = 0
    
    # Force cleanup before starting
    torch.cuda.empty_cache()
    for inputs, targets in loader:
        batch_count += 1
        
        # Move tensors to GPU one at a time
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)
        
        with torch.no_grad():  # Ensure no gradients are stored
            outputs = net(inputs)
            
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    final_accuracy = correct / total
    return final_accuracy


def compute_outputs(net, loader):
    """Auxiliary function to compute the logits for all datapoints.
    Does not shuffle the data, regardless of the loader.
    """

    # Make sure loader does not shuffle the data
    if isinstance(loader.sampler, torch.utils.data.sampler.RandomSampler):
        loader = DataLoader(
            loader.dataset, 
            batch_size=loader.batch_size, 
            shuffle=False, 
            num_workers=loader.num_workers)
    
    all_outputs = []
    
    for inputs, targets in loader:
        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)

        logits = net(inputs).detach().cpu().numpy() # (batch_size, num_classes)
        
        all_outputs.append(logits)
        
    return np.concatenate(all_outputs) # (len(loader.dataset), num_classes)


def false_positive_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Computes the false positive rate (FPR)."""
    fp = np.sum(np.logical_and((y_pred == 1), (y_true == 0)))
    n = np.sum(y_true == 0)
    return fp / n


def false_negative_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Computes the false negative rate (FNR)."""
    fn = np.sum(np.logical_and((y_pred == 0), (y_true == 1)))
    p = np.sum(y_true == 1)
    return fn / p


# The SCORING dictionary is used by sklearn's `cross_validate` function so that
# we record the FPR and FNR metrics of interest when doing cross validation
SCORING = {
    'false_positive_rate': make_scorer(false_positive_rate),
    'false_negative_rate': make_scorer(false_negative_rate)
}

def cross_entropy_f(x):
    # To ensure this function doesn't fail due to nans, find
    # all-nan rows in x and substitude them with all-zeros.
    x[np.all(np.isnan(x), axis=-1)] = np.zeros(x.shape[-1])
    
    pred = torch.tensor(np.nanargmax(x, axis = -1))
    x = torch.tensor(x)

    fn = nn.CrossEntropyLoss(reduction="none")
    
    return fn(x, pred).numpy()

def logistic_regression_attack(
        outputs_U, outputs_R, n_splits=2, random_state=0):
    """Computes cross-validation score of a membership inference attack.

    Args:
      outputs_U: numpy array of shape (N)
      outputs_R: numpy array of shape (N)
      n_splits: int
        number of splits to use in the cross-validation.
    Returns:
      fpr, fnr : float * float
    """
    assert len(outputs_U) == len(outputs_R)
    
    samples = np.concatenate((outputs_R, outputs_U)).reshape((-1, 1))
    labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))

    attack_model = linear_model.LogisticRegression()
    cv = model_selection.StratifiedShuffleSplit(
        n_splits=n_splits, random_state=random_state
    )
    scores =  model_selection.cross_validate(
        attack_model, samples, labels, cv=cv, scoring=SCORING)
    
    fpr = np.mean(scores["test_false_positive_rate"])
    fnr = np.mean(scores["test_false_negative_rate"])
    
    return fpr, fnr


def best_threshold_attack(
        outputs_U: np.ndarray, 
        outputs_R: np.ndarray, 
        random_state: int = 0
    ) -> tuple[list[float], list[float]]:
    """Computes FPRs and FNRs for an attack that simply splits into 
    predicted positives and predited negatives based on any possible 
    single threshold.

    Args:
      outputs_U: numpy array of shape (N)
      outputs_R: numpy array of shape (N)
    Returns:
      fpr, fnr : list[float] * list[float]
    """
    assert len(outputs_U) == len(outputs_R)
    
    samples = np.concatenate((outputs_R, outputs_U))
    labels = np.array([0] * len(outputs_R) + [1] * len(outputs_U))

    N = len(outputs_U)
    
    fprs, fnrs = [], []
    for thresh in sorted(list(samples.squeeze())):
        ypred = (samples > thresh).astype("int")
        fprs.append(false_positive_rate(labels, ypred))
        fnrs.append(false_negative_rate(labels, ypred))
    
    return fprs, fnrs

def compute_epsilon_s(fpr: list[float], fnr: list[float], delta: float) -> float:
    """Computes the privacy degree (epsilon) of a particular forget set example, 
    given the FPRs and FNRs resulting from various attacks.
    
    The smaller epsilon is, the better the unlearning is.
    
    Args:
      fpr: list[float] of length m = num attacks. The FPRs for a particular example. 
      fnr: list[float] of length m = num attacks. The FNRs for a particular example.
      delta: float
    Returns:
      epsilon: float corresponding to the privacy degree of the particular example.
    """
    assert len(fpr) == len(fnr)
    
    per_attack_epsilon = [0.]
    for fpr_i, fnr_i in zip(fpr, fnr):
        if fpr_i == 0 and fnr_i == 0:
            per_attack_epsilon.append(np.inf)
        elif fpr_i == 0 or fnr_i == 0:
            pass # discard attack
        else:
            with np.errstate(invalid='ignore'):
                epsilon1 = np.log(1. - delta - fpr_i) - np.log(fnr_i)
                epsilon2 = np.log(1. - delta - fnr_i) - np.log(fpr_i)
            if np.isnan(epsilon1) and np.isnan(epsilon2):
                per_attack_epsilon.append(np.inf)
            else:
                per_attack_epsilon.append(np.nanmax([epsilon1, epsilon2]))
            
    return np.nanmax(per_attack_epsilon)


def bin_index_fn(
        epsilons: np.ndarray, 
        bin_width: float = 0.5, 
        B: int = 13
        ) -> np.ndarray:
    """The bin index function."""
    bins = np.arange(0, B) * bin_width
    return np.digitize(epsilons, bins)


def F(epsilons: np.ndarray) -> float:
    """Computes the forgetting quality given the privacy degrees 
    of the forget set examples.
    """
    ns = bin_index_fn(epsilons)
    hs = 2. / 2 ** ns
    return np.mean(hs)

def forgetting_quality(
        outputs_U: np.ndarray, # (N, S)
        outputs_R: np.ndarray, # (N, S)
        attacks: list[Callable] = [logistic_regression_attack],
        delta: float = 0.01
    ):
    """
    Both `outputs_U` and `outputs_R` are of numpy arrays of ndim 2:
    * 1st dimension coresponds to the number of samples obtained from the 
      distribution of each model (N=512 in the case of the competition's leaderboard) 
    * 2nd dimension corresponds to the number of samples in the forget set (S).
    """
    
    # N = number of model samples
    # S = number of forget samples
    N, S = outputs_U.shape
    
    assert outputs_U.shape == outputs_R.shape, \
        "unlearn and retrain outputs need to be of the same shape"
    
    epsilons = []
    pbar = tqdm(range(S))
    for sample_id in pbar:
        pbar.set_description("Computing F...")
        
        sample_fprs, sample_fnrs = [], []
        
        for attack in attacks: 
            uls = outputs_U[:, sample_id]
            rls = outputs_R[:, sample_id]
            
            fpr, fnr = attack(uls, rls)
            
            if isinstance(fpr, list):
                sample_fprs.extend(fpr)
                sample_fnrs.extend(fnr)
            else:
                sample_fprs.append(fpr)
                sample_fnrs.append(fnr)
        
        sample_epsilon = compute_epsilon_s(sample_fprs, sample_fnrs, delta=delta)
        epsilons.append(sample_epsilon)
        
    return F(np.array(epsilons))

def _evaluate_dev(method, 
                  delta: float = 0.01, 
                  f: Callable = cross_entropy_f, 
                  attacks: list[Callable] = [best_threshold_attack, logistic_regression_attack]):
    """Development phase evaluation using CIFAR-10."""
    print("Loading CIFAR-10 data and models...")
    
    # Load data and models
    data_loaders = get_cifar10_data()
    pretrained_models = get_cifar10_pretrained_models(device=DEVICE)

    retain_loader = data_loaders["retain"]
    forget_loader = data_loaders["forget"] 
    val_loader = data_loaders["validation"]
    test_loader = data_loaders["testing"]

    original_model = pretrained_models["original"]
    retrained_model = pretrained_models["retrained"]

    # Create results directory if it doesn't exist
    results_dir = "dev_results"
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, f"{method.get_name()}_results.npz")

    outputs_U = []
    retain_accuracy = []
    test_accuracy = []
    forget_accuracy = []

    pbar = tqdm(range(NUM_MODELS))
    for i in pbar:
        u_model = deepcopy(original_model)
        
        pbar.set_description("Unlearning...")
        method.run(
            net=u_model,
            retain_loader=retain_loader,
            forget_loader=forget_loader,
            val_loader=val_loader
        )

        outputs_Ui = compute_outputs(u_model, forget_loader)
        outputs_U.append(f(outputs_Ui))

        pbar.set_description("Computing retain accuracy...")
        retain_accuracy.append(accuracy(u_model, retain_loader))

        pbar.set_description("Computing test accuracy...")
        test_accuracy.append(accuracy(u_model, test_loader))

        pbar.set_description("Computing forget accuracy...")
        forget_accuracy.append(accuracy(u_model, forget_loader))

    outputs_U = np.array(outputs_U)
    assert outputs_U.shape == (NUM_MODELS, len(forget_loader.dataset))

    RAR = accuracy(retrained_model, retain_loader)
    TAR = accuracy(retrained_model, test_loader) 
    FAR = accuracy(retrained_model, forget_loader)

    RAU = np.mean(retain_accuracy)
    TAU = np.mean(test_accuracy)
    FAU = np.mean(forget_accuracy)

    scale = np.std(outputs_U) / 10.
    outputs_Ri = compute_outputs(retrained_model, forget_loader)
    outputs_Ri = np.expand_dims(outputs_Ri, axis=0)
    outputs_R = np.random.normal(loc=outputs_Ri, scale=scale, size=(NUM_MODELS, *outputs_Ri.shape[-2:]))
    outputs_R = np.array([f(oRi) for oRi in outputs_R])

    forget_score = forgetting_quality(outputs_U, outputs_R, attacks=attacks, delta=delta)
    final_score = forget_score * (RAU / RAR) * (TAU / TAR)

    results = {
        "total_score": final_score,
        "forgetting_quality": forget_score,
        "unlearn_retain_accuracy": RAU, 
        "unlearn_test_accuracy": TAU,
        "unlearn_forget_accuracy": FAU,
        "retrain_retain_accuracy": RAR,
        "retrain_test_accuracy": TAR,
        "retrain_forget_accuracy": FAR
    }

    np.savez(results_file, **results)

    print(f"\nDetailed Results:")
    print(f"Forgetting Quality: {forget_score:.4f}")
    print(f"\nAccuracy Metrics:")
    print(f"  Unlearned Model:")
    print(f"    Retain Accuracy: {RAU:.4f}")
    print(f"    Test Accuracy: {TAU:.4f}")
    print(f"    Forget Accuracy: {FAU:.4f}")
    print(f"\n  Retrained Model:")
    print(f"    Retain Accuracy: {RAR:.4f}")
    print(f"    Test Accuracy: {TAR:.4f}")
    print(f"    Forget Accuracy: {FAR:.4f}")
    print(f"\nRatios:")
    print(f"  Retain Accuracy (RAU/RAR): {RAU/RAR:.4f}")
    print(f"  Test Accuracy (TAU/TAR): {TAU/TAR:.4f}")
    print(f"\nFinal Score: {final_score:.4f}")

    return results

###### TEST PHASE ######

def update_metadata(target_file, new_notebook_name):
   # Read target metadata
   with open(target_file, 'r') as f:
       target_metadata = json.load(f)

   # Update fields
   target_metadata.update({
       'id': f"armeddinosaur/{new_notebook_name}",
       'title': new_notebook_name,
       'code_file': f"{new_notebook_name}.ipynb",
       'language': 'python',
       'kernel_type': 'notebook',
       'is_private': True,
       'enable_gpu': True, 
       'enable_tpu': False,
       'enable_internet': False,
       'competition_sources': ['neurips-2023-machine-unlearning']
   })

   # Write updated metadata
   with open(target_file, 'w') as f:
       json.dump(target_metadata, f, indent=1)

def method_to_notebook_code(method):
    """Extract run() implementation and format for notebook."""
    source = inspect.getsource(method.run)
    body = "\n    ".join(line for line in source.split("\n")[1:])
    min_indent = min(len(line) - len(line.lstrip()) 
                    for line in body.split("\n") if line.strip())
    body = "\n".join(line[min_indent:] for line in body.split("\n"))
    
    return f'''def unlearning(
    net, 
    retain_loader, 
    forget_loader, 
    val_loader):
    """Generated from {method.get_name()} implementation."""
    {body}'''

def _evaluate_test(method):
    """Test phase evaluation using existing notebook template."""
    if os.path.exists("test_submission"):
        shutil.rmtree("test_submission")
    notebook_path = "data/run-unlearn-finetune.ipynb"
    output_dir = os.path.join("test_submission")
    os.makedirs(output_dir, exist_ok=True)
    
    # Read template notebook
    with open(notebook_path, 'r') as f:
        nb = nbformat.read(f, as_version=4)
    
    # Extract and replace unlearning function
    for cell in nb.cells:
        if "def unlearning" in cell.source:
            cell.source = method_to_notebook_code(method)
            break
            
    # Save modified notebook
    timestamp = int(time.time())
    output_notebook = os.path.join(output_dir, f"submission_{timestamp}.ipynb")
    with open(output_notebook, 'w') as f:
        nbformat.write(nb, f)

    # Initialize kernel metadata
    os.system(f'kaggle kernels init -p "{output_dir}"')

    update_metadata(os.path.join(output_dir, 'kernel-metadata.json'), f"submission_{timestamp}")

    try:
        result = os.system(f'kaggle kernels push -p "{output_dir}"')
        if result != 0:
            print(f"Error pushing kernel: exit code {result}")
            raise Exception(f"Kernel push failed with exit code {result}")
    except Exception as e:
        print(f"Exception occurred while pushing kernel: {str(e)}")
        raise
    print(f"Kernel pushed successfully. Notebook ID: {timestamp} \n Please submit by going to the competition page and clicking on late submission. \n this notebook with version 1 and submission.zip should be visible.")

###### GET SCORE ######

def get_score(method, phase: str = "dev") -> float:
    """Get evaluation score."""
    if phase == "dev":
        # Load cached results from file
        results_dir = "dev_results"
        results_file = os.path.join(results_dir, f"{method.get_name()}_results.npz")
        if os.path.exists(results_file):
            results = np.load(results_file)
            return float(results["total_score"])
    
    # For test phase
    test_dir = "test_submission"
    notebooks = [f for f in os.listdir(test_dir) if f.startswith('submission_')]
    if not notebooks:
        return None
        
    # Get timestamp from filename
    latest_notebook = sorted(notebooks)[-1]  # Get most recent
    timestamp = int(latest_notebook.split('_')[1].split('.')[0])
    return float(timestamp)