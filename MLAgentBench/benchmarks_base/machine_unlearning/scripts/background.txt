**The Fanchuan method** first iterates over the entire forget set once and performs one step per mini-batch towards minimizing the KL-divergence between the predictions and a uniform distribution \(\downarrow \text{KL}[f(x_u) \| \text{uniform}]\). It then iterates for 8 epochs over the forget set and performs gradient ascent steps towards maximizing a (temperature-mitigated) dot-product contrastive loss between the forget set mini-batch and a mini-batch of randomly-sampled retain set examples \(\uparrow \text{contrastive}(f(x_r), f(x_u))\). After each contrastive epoch, it performs one epoch of categorical cross-entropy training on the retain set \(\downarrow \text{CE}(f(x_r), y_r)\).

---

**The Kookmin method** reinitializes a subset of the model weights (\(\text{Reinit } \theta \subseteq \theta_0\)) before finetuning on the retain set \(\downarrow \text{CE}(f(x_r), y_r)\). The parameters to be reinitialized are decided based on the gradient magnitude of the NegGrad+ loss over the forget and retain sets. The convolutional weights with the bottom 30% gradient magnitudes are reinitialized. During finetuning, the gradients of the reinitialized and remaining convolutional parameters are multiplied by 1.0 and 0.1, respectively.

---

**The Seif method** adds Gaussian noise (\(\mu = 0, \sigma = 0.6\)) to convolutional weights \(\theta \sim \mathcal{N}(\mu = \theta_0, \sigma^2 \cdot I)\) and performs 4 epochs of finetuning using a cross-entropy loss \(\downarrow \text{CE}(f(x_r), y_r)\), the magnitude of which is adjusted based on the number of majority class examples present in the mini-batch. Rather than directly averaging the examplewise losses in the mini-batch, the Seif method computes a weighted average of the examplewise losses using a weight of 1.0 for majority class examples and a weight of 0.05 for other examples. This is equivalent to using a learning rate which depends on the number of majority class examples in the mini-batch. Before the final epoch, additive Gaussian noise (\(\mu = 0, \sigma = 0.005\)) is applied to the convolutional weights.

---

**The Sebastian method** reinitializes a significant portion (99%) of the convolutional and fully-connected layer weights with the lowest \(L_1\) norm (\(\text{Reinit } \theta \subseteq \theta_0\)), then performs finetuning on the retain set \(\downarrow \text{CE}(f(x_r), y_r) + \text{MSE} \bigl(H(f(x_r)), H(f_0(x_r))\bigr)\) using a combination of cross-entropy and mean squared error between the model prediction’s entropy \(H(f(x_r))\) and that of the original model \(H(f_0(x_r))\).

---

**The Amnesiacs method** reinitializes the first convolutional layer and the fully-connected layer (\(\text{Reinit } \theta \subseteq \theta_0\)) before performing 3 “warmup” epochs of distilling the original model’s predictions \(f_0(x_v)\) for a held-out validation set into the reinitialized model \(\downarrow \text{KL}[f(x_v) \| f_0(x_v)]\). The method then performs an additional 3 epochs of finetuning on the retain set using a combination of cross-entropy loss \(\text{CE}(f(x_r), y_r)\) and symmetric KL-divergence loss \(\text{KL}_\text{sym}[f(x_r) \| f_0(x_r)]\) between the model’s predictions \(f(x_r)\) and the original model’s predictions \(f_0(x_r)\).

---

**The Sun method** reinitializes the fully-connected layer (\(\text{Reinit } \theta \subseteq \theta_0\)), then performs several epochs of “noised” finetuning on the retain set. Before each such epoch, a random subset of layers (excluding batch normalization) is selected and additive Gaussian noise is applied to their parameters \(\theta \sim \mathcal{N}(\mu = \theta_0, \sigma^2 \cdot I)\). These selected layers are then finetuned for an epoch. Finally, the model is finetuned normally on the retain set for a few epochs.

---

**The Forget method** iterates over several cycles of:
1. Reinitializing a random subset of layers (\(\text{Reinit } \theta \subseteq \theta_0\)).
2. Distilling the original model’s predictions on the forget set into the reinitialized model for an epoch using a mean squared error loss \(\downarrow \text{MSE}(f_0(x_r), f(x_r))\).

---

References:
[1] Triantafillou, E., Kairouz, P., Pedregosa, F., Hayes, J., Kurmanji, M., Zhao, K., ... & Guyon, I. (2024). Are we making progress in unlearning? Findings from the first NeurIPS unlearning competition. arXiv preprint arXiv:2406.09073.
