# Machine Unlearning Challenge

**One-sentence summary**  
Develop efficient algorithms for “machine unlearning” such that, after forgetting certain training data, the resulting model closely matches one that was never trained on that data in the first place.

---

## Description

We focus on **machine unlearning**, i.e., “removing the influence” of a subset of the training data (the *forget set*) from a trained model, so that the resulting model behaves similarly to one trained *without* that subset. This is especially relevant for privacy regulations (e.g., “right to be forgotten”), where individuals can request removal of their data from a model.

### Goal

Our goal is to compare the strengths and weaknesses of different unlearning methods under a *shared* and *standardized* evaluation. Participants receive:

1. A **pre-trained** model (trained on facial images, CASIA-SURF, to predict age group in test phase, CIFAR-10 in dev phase).  
2. A **forget set** (data samples to remove) and a **retain set** (the rest of training data).  
3. A hidden **test set** for final scoring.

**Output**: An unlearned model that should:
- **Erase** the forget set’s influence to match the behavior of a retrained model that never saw those forget samples.  
- **Retain** good accuracy on the remaining data and on the test set.  
- **Finish** within provided compute/runtime constraints.

### Data & Evaluation

- **Dataset**: CASIA-SURF, containing facial images labeled by age group (10 classes) in test phase, CIFAR-10 in dev phase.  
- **Pretrained model**: A classifier trained for 30 epochs on the entire dataset.  
- **Forgetting**: Must “remove” any trace of the forget set.  
- **Utility**: Must stay accurate on the retain data and a hidden test set.  
- **Metrics**:  
  1. **Forgetting quality** – compares unlearned model \(\theta_u\) to a model retrained from scratch \(\theta_r\) without the forget set.  
  2. **Utility** – checks retain/test accuracy relative to \(\theta_r\).  
  3. **Efficiency** – run under time constraints (< 8h on provided compute).

The challenge uses an *online* evaluation on Kaggle. Each submitted unlearning method will be run multiple times against multiple “original” and “retrained-from-scratch” checkpoints, producing a final score that balances forgetting quality and model utility.

---

## Developing New Methods

We provide a **starter kit** with:
- Data loading scripts and sample code.  
- Baseline example method (`MyMethod.py`), which simply fine-tunes the original model on the retain set for a few epochs.

**Baseline Method**  
The baseline starts with the full, pretrained model and “forgets” by running a few epochs of fine-tuning on the retain set. It relies on *catastrophic forgetting* and is not expected to excel on our detailed unlearning metric, but it serves as an easy-to-follow template for your custom solutions.

### Steps to Add a New Method

1. **Create a new file** in `methods/` (e.g. `MyGreatUnlearning.py`).  
2. **Inherit** from `BaseMethod.py` (implementing or overriding `__init__()` and `run()`).  
3. **Register** it in `methods/__init__.py`:  
   - Add an entry returning your class in the `all_method_handlers()` dictionary.  
   - Import your new file so it is recognized by `main.py`.  

### Important Note About Method Implementation
Your method's `run()` function should be written in a way that can be directly copied into the notebook's `unlearning()` function. Key points:
- Keep the same parameter signature: `(net, retain_loader, forget_loader, val_loader)`
- Avoid using additional parameters like `phase`
- Use in-place model modifications (the notebook expects this)
- Include all necessary cleanup code
- Match the notebook's function structure

Please review the provided run-unlearn-finetune.ipynb notebook to understand the execution environment and ensure your implementation is compatible with it.

The code conversion is handled by notebook_utils.py, which is available for modification. By default, it extracts just the run() function implementation, but developers can modify this file to support their preferred code organization approach.

Note: While notebook_utils.py can be customized to accommodate different coding patterns, ensure your modifications maintain compatibility with the notebook environment and competition requirements.

The evaluation pipeline will automatically convert your method's `run()` implementation into the notebook format.

Feel free to experiment with:
- Gradient-based unlearning (e.g. gradient ascent on forget set + standard training on retain set).  
- Layer re-initialization or parameter pruning.  
- Noise injection on certain parameters or layers.  
- Other creative approaches that meaningfully remove the forget set’s influence yet preserve accuracy on retain/test.

---

## Test Method
After adding your new file in `methods/`, you can test locally:
Simply run `python main.py -m {method_name}`. For example, to test the baseline method, execute `python main.py -m my_method`.
## What Happens

### Unlearning Step
1. **Loads** the original pretrained model and your unlearning code.  
2. **Applies** the “forget” procedure to produce an unlearned model.

### Evaluation
1. **Compares** your unlearned model against a reference model \(\theta_r\) that is retrained-from-scratch **without** the forget data.  
2. **Computes** metrics related to forgetting quality and utility (retain/test accuracy).  
3. **Aggregates** these into a **final score**.

Your local test run uses a CIFAR10 for quick checks. The official Kaggle scoring runs your approach **multiple times** with different seeds, generating multiple unlearned models for a more robust final metric.

---

## Competition Rules

### Focus on Novel Algorithms
- **Aim** for genuinely new unlearning approaches (e.g., multi-phase re-initialization, specialized objective, custom scheduling).  
- **Not allowed**: trivial “prompting,” ignoring the forgetting step, or simply skipping training.

### Compute Constraints
- **CPU/GPU** usage must finish within 8 hours total.  
- **Internet access** is disabled during evaluation.  
- Publicly available data/models can be used if **bundled offline** in the submitted notebook.

### No Full Retraining
- **Do not** just retrain from scratch on the retain set (that defeats the purpose of **approximate** unlearning).  
- Partial re-init or partial retraining is fine, but must be **significantly cheaper** than naive full retraining.

### Submission Requirements
- Must produce a **final unlearned checkpoint** that excludes the forget set’s influence.  
- Must adhere to **Kaggle’s code-of-conduct**.

We look forward to your innovative solutions—**happy unlearning**!


	## Research Ideas
	You will be provided with some research ideas on this problem from a machine learning expert. You’re encouraged to draw inspiration from these ideas when implementing your own method.

	Title: “Progressive Saliency-Guided Machine Unlearning for Resource-Constrained Scenarios”

Origins and Motivation:  
This idea arises from the need to comply with privacy regulations such as the “right to be forgotten,” described in “Comprehensive Study of Automatic Speech Emotion Recognition Systems” (Paper 0) and reinforced by the standardized challenge in “Towards Improving Speech Emotion Recognition Using Synthetic Data Augmentation from Emotion Conversion” (Paper 1). Existing methods can struggle to remove data influences quickly without harming overall accuracy, as shown by “Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy” (Paper 2). Although “SA LUN: EMPOWERING MACHINE UNLEARNING VIA GRADIENT-BASED WEIGHT SALIENCY…” (Paper 3) demonstrates promising results with gradient-based weight saliency, it can require extensive computations for complex forget sets. Furthermore, the efficiency insights from “Machine Unlearning in Learned Databases: An Experimental Analysis” (Paper 4) have not been widely applied back to real-time classification tasks. Together, these gaps motivate a method that unlearns data in finer, carefully controlled steps while staying efficient.

Novelty and Differentiation:  
Our approach implements unlearning in multiple short, targeted rounds rather than a single procedure. Each round identifies and modifies the model parameters most responsible for the “forget set,” as guided by a saliency map similar to that in Paper 3. However, rather than attempting to unlearn everything at once, we reduce influence incrementally. This introduces:  
• Progressive Weight Refinement: Breaking the unlearning process into multiple passes allows for low-risk adjustments after each round, preserving performance on the retain set.  
• Adaptive Efficiency Policies: Inspired by Paper 4’s resource-sensitive strategies, we adjust the number of unlearning rounds based on real-time metrics, preventing unnecessary computational overhead.  
• Fine-Tuned Gradient Updates: We incorporate short fine-tuning steps after each unlearning round, using only selective gradients, to maintain stable accuracy on the retain set.

Challenges and How We Overcome Them:  
1. Minimizing Catastrophic Forgetting: Completely removing influence from the forget set can risk damaging the model’s accuracy on other data. We address this by applying partial updates—only to the saliency-identified weights—followed by brief fine-tuning on the retain set.  
2. Balancing Speed and Thoroughness: Unlearning methods often suffer from long computation times or incomplete removal. Our multi-pass approach and adaptive policies determine the minimum number of rounds needed to reach a satisfactory level of “forget set” removal, reducing wasted effort.  
3. Preserving Model Utility Under Constraints: Large-scale models may need to unlearn thousands of samples while meeting strict runtime limits. By refining fewer parameters per round and leveraging selective gradient strategies, we meet these time constraints without full retraining.

Proposed Method:  
1. Saliency Mapping: Estimate which parameters have the strongest connections to the forget set.  
2. Iterative Modification: Adjust those parameters in a series of short rounds, reducing the probability of over-fitting or harming non-forgotten data.  
3. Stability Fine-Tuning: After each unlearning round, perform a restrained “rehearsal” using the retain set to preserve key features.  
4. Termination/Policy Check: Monitor forgetting metrics and runtime. Conclude the unlearning process once the model matches a retrained-from-scratch baseline or upon hitting maximum rounds.  
5. Final Evaluation: Validate overall performance on a hidden test set to ensure that forgetting quality, model utility, and runtime efficiency align with challenge benchmarks.

This progressive, saliency-guided strategy offers a practical path forward for machine unlearning, balancing thorough data removal with the need to maintain accuracy and remain computationally feasible.
	


	## Research Ideas
	You will be provided with some research ideas on this problem from a machine learning expert. You’re encouraged to draw inspiration from these ideas when implementing your own method.

	Title: “Adaptive Multi-Step Saliency-Based Unlearning (AMS-Unlearn)”

Origins and Motivation:
Machine unlearning aims to remove a model’s memory of specific data (the “forget set”) without sacrificing overall performance. Prior works, including “Machine unlearning: linear filtration for logit-based classifiers,” have proposed single-step solutions that adjust model parameters once to eliminate unwanted information. However, real-world settings often require repeated forgetting of new data samples over time. Inspired by “SA LUN: EMPOWERING MACHINE UNLEARNING VIA GRADIENT-BASED WEIGHT SALIENCY IN BOTH IMAGE CLASSIFICATION AND GENERATION,” we propose an approach that repeatedly identifies and updates only the most influential parameters, making unlearning feasible whenever new forget requests arrive.

Novelty and Key Differences:
Unlike existing methods that handle unlearning as a singular process, AMS-Unlearn offers a multi-step strategy to continually “forget” newly specified data. It does so by maintaining and recalculating a saliency map of network parameters that tracks which weights most strongly influence the model's decisions on the forget samples. The approach takes inspiration from the incremental adaptation ideas surfaced in “Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy,” but focuses on refining saliency-based updates to avoid full-scale retraining with every request. This reduces computational overhead, aligning with the efficiency goals highlighted in “Machine Unlearning Will Change Medicine.”

Challenges and How We Overcome Them:
1. Maintaining Accuracy: Each new unlearning step might harm the model’s ability to handle data outside the forget set. To address this, AMS-Unlearn iteratively fine-tunes only the saliency-selected weights, preserving accuracy on the retained data.  
2. Handling Multiple Requests: Frequent requests can accumulate computational costs. Our method builds a dynamic saliency map that updates gradually, focusing on the most critical regions of the model. This selective adjustment reduces the runtime overhead, advancing ideas from “Machine Unlearning in Learned Databases: An Experimental Analysis.”  
3. Ensuring Robust Forgets: Continual modification of weights may leave traces of forgotten data. AMS-Unlearn’s targeted recalibration checks the model’s outputs after each unlearning step, ensuring that the forgotten data’s influence is effectively removed at every iteration.

By balancing selective fine-tuning with multi-step adaptability, AMS-Unlearn empowers practical unlearning solutions under tight computational budgets. This incremental approach remains faithful to established unlearning metrics (accuracy, forgetting quality, and efficiency), providing a fresh direction for handling the ever-growing demand for “continuous” data removal while maintaining strong performance.
	


	## Research Ideas
	You will be provided with some research ideas on this problem from a machine learning expert. You’re encouraged to draw inspiration from these ideas when implementing your own method.

	Title: Hierarchical Impair-Repair with Saliency (HIRS) for Efficient Machine Unlearning

Origins & Motivation:  
Several approaches to machine unlearning have been proposed to meet the "right to be forgotten," including logit-based approaches (see "Machine unlearning: linear filtration for logit-based classifiers") and quick weight impair-repair solutions (see "Fast Yet Effective Machine Unlearning"). However, many methods rely on either a single high-level adjustment (which may only hide rather than fully remove unwanted information) or overly broad weight modifications (potentially harming essential model knowledge). Inspired by these limitations, HIRS aims to blend efficient logit filtration with fine-grained parameter updates, ensuring fast and thorough unlearning while preserving the model’s accuracy on retain data.

Novelty & Differences from Previous Work:  
• Two-Tier Process: HIRS combines a rapid "Impair" step at the logit level with a subsequent "Repair" step that targets only the parameters deemed crucial—based on gradient-based saliency maps (as introduced in "SA LUN: EMPOWERING MACHINE UNLEARNING VIA GRADIENT-BASED WEIGHT SALIENCY IN BOTH IMAGE CLASSIFICATION AND GENERATION").  
• Balanced Efficiency and Depth: Unlike single-pass filtering methods, HIRS conducts an inexpensive initial pass to quickly diminish the forget set’s influence, followed by a smaller, more precise parameter adjustment procedure.  
• Broader Applicability: By focusing on adjustable design choices (e.g., how many parameters to repair), HIRS can adapt readily to various model architectures, including deeper networks often underserved by purely logit-level fixes.

Method Overview:  
1) Impair (Logit Adjustment): Shortly after identifying the forget set, HIRS applies a linear function at the logit layer to diminish patterns tied to the forget data. This is quick to compute and aims to remove most traces of the forget set.  
2) Repair (Saliency-Guided Fine-Tuning): Next, a gradient-based saliency map identifies parameters significantly linked to the forget set’s influence. Only these salient parameters are fine-tuned using the retain set, neatly restoring model accuracy. This selective approach avoids full retraining, maintaining high efficiency.

Challenges & How They Are Addressed:  
• Maintaining Accuracy: The two-stage approach ensures minimal disturbance—broad neutralization is corrected by targeted fine-tuning.  
• Scalability: Because the Impair step uses lightweight logit transformations and the Repair step updates only selected parameters, HIRS is designed to fit under stringent time limits for large-scale tasks.  
• Retaining Interpretability: Using linear filtrations at the logit layer keeps the unlearning process transparent, while saliency-based weight fine-tuning informs which parameters matter most.

By unifying the speed of logit-based unlearning with the precision of saliency-guided repairs, HIRS aims to deliver a solid balance of forgetting quality, retained accuracy, and computational feasibility—pushing forward the ongoing research in machine unlearning.
	


	## Research Ideas
	You will be provided with some research ideas on this problem from a machine learning expert. You’re encouraged to draw inspiration from these ideas when implementing your own method.

	Title: Federated Targeted Saliency Unlearning (FTSU) for Efficient and Certified Privacy Preservation

Origins & Motivation:
Federated learning enables multiple devices or institutions to collaboratively train a model without sharing raw data. However, when a user requests the removal of their personal data, existing unlearning solutions either require intensive retraining or involve significant client-side interaction and storage. Inspired by “Forgettable Federated Linear Learning with Certified Data Removal,” which uses linear approximations for fast data removal on the server side, and “SA LUN: EMPOWERING MACHINE UNLEARNING VIA GRADIENT-BASED WEIGHT SALIENCY,” which highlights the benefits of targeting specific model weights, our aim is to merge these insights into a federated framework that is both efficient and effective at forgetting.

Novelty & Difference from Previous Works:
1. Hybrid Saliency & Linear Approximations: Unlike traditional methods that rely solely on either gradient reuse (e.g., FedRemoval) or weight-level targeting (e.g., SalUn), FTSU applies a gradient-based saliency analysis within a certified linear approximation, ensuring precise yet computationally light unlearning.  
2. Localized “Impair–Repair” Steps: Inspired by “Fast Yet Effective Machine Unlearning,” FTSU performs a targeted impair step to degrade performance on the forget set, followed by a brief, localized repair focusing on the retain set. This dual-phase unlearning keeps overall accuracy high without requiring a full retrain.  
3. Minimal Client Overheads: FTSU integrates seamlessly with federated architectures by storing only a limited set of recent gradients on the server. Once unlearning is complete, the updated model is broadcast back without further client input, aligning with existing “Forgettable Federated Linear Learning” protocols.

Key Challenges & How They Are Overcome:
• Challenge 1: Identifying which parts of the model most strongly encode the forget set.  
  – Overcome by applying a gradient-based saliency map that pinpoints the most influential parameters, limiting unlearning changes to these critical weights.

• Challenge 2: Maintaining strong accuracy on retained data while forgetting the requested data.  
  – Overcome by adapting “impair–repair” ideas, which degrade performance specifically on the forget set, then quickly restore accuracy on the retain set through a repair step guided by linear approximation.

• Challenge 3: Achieving theoretical guarantees of effective unlearning.  
  – Overcome by using the certified unlearning principles from “Forgettable Federated Linear Learning with Certified Data Removal,” ensuring our final model is indistinguishable (with respect to the forget set) from a model never trained on the forgotten data.

Overall, FTSU stands out as a compact, saliency-driven approach that ensures user data requests are handled speedily and accurately, while federated constraints—like limited access to full data—are respected. It aims to preserve high model utility on retained data and provide formal proofs that forgotten data leaves no trace in the final model.
	


	## Research Ideas
	You will be provided with some research ideas on this problem from a machine learning expert. You’re encouraged to draw inspiration from these ideas when implementing your own method.

	**The Fanchuan method** first iterates over the entire forget set once and performs one step per mini-batch towards minimizing the KL-divergence between the predictions and a uniform distribution \(\downarrow \text{KL}[f(x_u) \| \text{uniform}]\). It then iterates for 8 epochs over the forget set and performs gradient ascent steps towards maximizing a (temperature-mitigated) dot-product contrastive loss between the forget set mini-batch and a mini-batch of randomly-sampled retain set examples \(\uparrow \text{contrastive}(f(x_r), f(x_u))\). After each contrastive epoch, it performs one epoch of categorical cross-entropy training on the retain set \(\downarrow \text{CE}(f(x_r), y_r)\).

---

**The Kookmin method** reinitializes a subset of the model weights (\(\text{Reinit } \theta \subseteq \theta_0\)) before finetuning on the retain set \(\downarrow \text{CE}(f(x_r), y_r)\). The parameters to be reinitialized are decided based on the gradient magnitude of the NegGrad+ loss over the forget and retain sets. The convolutional weights with the bottom 30% gradient magnitudes are reinitialized. During finetuning, the gradients of the reinitialized and remaining convolutional parameters are multiplied by 1.0 and 0.1, respectively.

---

**The Seif method** adds Gaussian noise (\(\mu = 0, \sigma = 0.6\)) to convolutional weights \(\theta \sim \mathcal{N}(\mu = \theta_0, \sigma^2 \cdot I)\) and performs 4 epochs of finetuning using a cross-entropy loss \(\downarrow \text{CE}(f(x_r), y_r)\), the magnitude of which is adjusted based on the number of majority class examples present in the mini-batch. Rather than directly averaging the examplewise losses in the mini-batch, the Seif method computes a weighted average of the examplewise losses using a weight of 1.0 for majority class examples and a weight of 0.05 for other examples. This is equivalent to using a learning rate which depends on the number of majority class examples in the mini-batch. Before the final epoch, additive Gaussian noise (\(\mu = 0, \sigma = 0.005\)) is applied to the convolutional weights.

---

**The Sebastian method** reinitializes a significant portion (99%) of the convolutional and fully-connected layer weights with the lowest \(L_1\) norm (\(\text{Reinit } \theta \subseteq \theta_0\)), then performs finetuning on the retain set \(\downarrow \text{CE}(f(x_r), y_r) + \text{MSE} \bigl(H(f(x_r)), H(f_0(x_r))\bigr)\) using a combination of cross-entropy and mean squared error between the model prediction’s entropy \(H(f(x_r))\) and that of the original model \(H(f_0(x_r))\).

---

**The Amnesiacs method** reinitializes the first convolutional layer and the fully-connected layer (\(\text{Reinit } \theta \subseteq \theta_0\)) before performing 3 “warmup” epochs of distilling the original model’s predictions \(f_0(x_v)\) for a held-out validation set into the reinitialized model \(\downarrow \text{KL}[f(x_v) \| f_0(x_v)]\). The method then performs an additional 3 epochs of finetuning on the retain set using a combination of cross-entropy loss \(\text{CE}(f(x_r), y_r)\) and symmetric KL-divergence loss \(\text{KL}_\text{sym}[f(x_r) \| f_0(x_r)]\) between the model’s predictions \(f(x_r)\) and the original model’s predictions \(f_0(x_r)\).

---

**The Sun method** reinitializes the fully-connected layer (\(\text{Reinit } \theta \subseteq \theta_0\)), then performs several epochs of “noised” finetuning on the retain set. Before each such epoch, a random subset of layers (excluding batch normalization) is selected and additive Gaussian noise is applied to their parameters \(\theta \sim \mathcal{N}(\mu = \theta_0, \sigma^2 \cdot I)\). These selected layers are then finetuned for an epoch. Finally, the model is finetuned normally on the retain set for a few epochs.

---

**The Forget method** iterates over several cycles of:
1. Reinitializing a random subset of layers (\(\text{Reinit } \theta \subseteq \theta_0\)).
2. Distilling the original model’s predictions on the forget set into the reinitialized model for an epoch using a mean squared error loss \(\downarrow \text{MSE}(f_0(x_r), f(x_r))\).

---

References:
[1] Triantafillou, E., Kairouz, P., Pedregosa, F., Hayes, J., Kurmanji, M., Zhao, K., ... & Guyon, I. (2024). Are we making progress in unlearning? Findings from the first NeurIPS unlearning competition. arXiv preprint arXiv:2406.09073.
	
