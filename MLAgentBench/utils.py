import json
import re
import difflib

from MLAgentBench.LLM import complete_text

def count_different_lines(code1: str, code2: str) -> int:
    """
    Compute the number of different non-empty lines between two code snippets.

    Parameters:
        code1 (str): The first code snippet.
        code2 (str): The second code snippet.

    Returns:
        int: The number of different non-empty lines.
    """
    # Split the code into lines
    code1_lines = code1.splitlines()
    code2_lines = code2.splitlines()

    # Generate the diff
    diff = difflib.unified_diff(code1_lines, code2_lines, lineterm='')

    # Count the lines starting with '+' or '-' (excluding headers and empty lines)
    diff_count = sum(
        1
        for line in diff
        if line.startswith(('+', '-')) 
        and not line.startswith(('+++', '---')) 
        and line[1:].strip()  # Exclude lines that are empty after the prefix
    )

    return diff_count

def count_code_lines(code: str) -> int:
    lines = code.splitlines()
    return sum(1 for line in lines if line.strip())  # Exclude empty lines

def sanitize_json_string(s):
    """ Try to sanitize a string to be a valid JSON string."""
    s = s.strip("```json").strip("```").strip()
    s = s.replace('\\', '\\\\')  # Escape backslashes first
    # s = s.replace('/', '\\/')  # Escape forward slashes
    s = s.replace('\b', '\\b')  # Escape backspaces
    s = s.replace('\f', '\\f')  # Escape form feeds
    s = s.replace('\r', '\\r')  # Escape carriage returns
    s = s.replace('\t', '\\t')  # Escape horizontal tabs
    # triple quotes are a problem
    return re.sub(r'"([^"]*)"', lambda m: '"' + m.group(1).replace('\n', '\\n').replace('\"', '\\"') + '"', s)

FEEDBACK_MODEL = "gpt-4o-mini"
FEEDBACK_MAX_TOKENS = 4000
MAX_RETRYS = 5
def get_llm_feedback(idea, code):
    prompt = f"""You have a research idea proposal and the corresponding code generated by AI. Your task is to evaluate how faithfully the code implements the proposed idea. Specifically, you should:

    1. Identify the Method: Identify the core portion of the proposal that describes the methodology, ignoring the other parts such as abstract, motivation, impact, etc.
    2. Identify the Code: Identify the core portion of code that implements the method, ignoring the other parts such as class initialization, loading dataset/model/tokenizer, setting hyperparameters, etc. 
    3. Segment the Text and Code: Break down both the method text and the code into minimal, meaningful units.
    4. Match Units: For each unit of text, find the corresponding unit of code that is relevant to it.
    5. Judgement of Relevance: For each matched pair, provide a judgement of relevance, categorizing it as "high", "medium", "low", or "unsure", and include your rationale for that judgement.

    Format your output in JSON: The output should be a list of dictionaries, where each dictionary contains the following entries:
    - "text": The text unit from the method.
    - "code": The corresponding code unit.
    - "rationale": Your reasoning for the relevance judgement.
    - "relevance": Your relevance judgement.

    If a unit of text does not have a corresponding code unit or vice versa, create a dictionary entry in the output that leaves the unmatched field blank.

    # Idea Proposal

    {idea}

    # Code

    ```python
    {code}
    ```"""
    i = 0
    items = None
    while i < MAX_RETRYS: 
        i += 1
        completion = complete_text(prompt, log_file=None, model=FEEDBACK_MODEL, max_tokens_to_sample=FEEDBACK_MAX_TOKENS) 
        completion = sanitize_json_string(completion)

        try:
            items = json.loads(completion)
            feedback = ""
            relevance_score, total_units = 0, 0 
            for item in items:
                if item['relevance'] == "high":
                    relevance_score += 1
                    total_units += 1
                elif item['relevance'] == "medium":
                    relevance_score += 0.5
                    total_units += 1
                elif item['relevance'] == "low":
                    relevance_score += 0
                    total_units += 1

                if item['relevance'] in ["medium", "low"]:
                    # check if all pairs are relevant, then feedback is empty string
                    method = item['text'] if item['text'] else "None"
                    code_snippet = f"```python\n{item['code']}\n```" if item['code'] else "None"
                    feedback += f"Method Description: {method}\n"
                    feedback += f"Code Implementation:\n{code_snippet}\n"
                    feedback += f"Feedback: The relevance between this code snippet and the described method is {item['relevance']}. {item['rationale']}\n\n"
            if feedback:
                feedback_prefix = "\nHere is the feedback for how some parts of your code may not faithfully reflect the proposed method from another AI. Please improve your code based on the feedback.\n\n"
                feedback = feedback_prefix + feedback
            if total_units:
                relevance_score = relevance_score / total_units
            else:
                relevance_score = None
            return feedback, relevance_score  
        except Exception as e:
            # print(f"DEBUG: error parsing -- {e}\n", completion)
            continue

    return "", None
